{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Fri Aug 21 00:56:20 2020\n", "@filename: utils.py\n", "@describe: utility functions\n", "@dataset: Nil\n", "@author: cyruslentin\n", "\"\"\"\n", "import numpy as np\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["space count per coulmn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def colSpaceCount(colName):\n", "    \"\"\"\n", "    returns: \n", "        number of rows which contain <blank>\n", "    usage: \n", "        colSpaceCount(colName)\n", "    \"\"\" \n", "    return (colName.str.strip().values == '').sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["space count for data frame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def SpaceCount(df): \n", "    \"\"\"\n", "    returns:  \n", "        number of rows which contain <blank> iterating through each col of df\n", "    usage: \n", "        SpaceCount(df)\n", "    \"\"\"\n", "    colNames = df.columns\n", "    dsRetValue = pd.Series() \n", "    for colName in colNames:\n", "        if df[colName].dtype == \"object\": \n", "            dsRetValue[colName] = colSpaceCount(df[colName])\n", "    return(dsRetValue)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["outlier count for column"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def colOutCount(colValues):\n", "    \"\"\"\n", "    returns: \n", "        count of outliers in the colName\n", "    usage: \n", "        colOutCount(colValues)\n", "    \"\"\"\n", "    quartile_1, quartile_3 = np.percentile(colValues, [25, 75])\n", "    iqr = quartile_3 - quartile_1\n", "    lower_bound = quartile_1 - (iqr * 3.0)\n", "    upper_bound = quartile_3 + (iqr * 3.0)\n", "    ndOutData = np.where((colValues > upper_bound) | (colValues < lower_bound))\n", "    ndOutData = np.array(ndOutData)\n", "    return ndOutData.size"]}, {"cell_type": "markdown", "metadata": {}, "source": ["outlier count for dataframe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def OutlierCount(df): \n", "    \"\"\"\n", "    returns: \n", "        count of outliers in each column of dataframe\n", "    usage: \n", "        OutlierCount(df): \n", "    \"\"\"\n", "    colNames = df.columns\n", "    dsRetValue = pd.Series() \n", "    for colName in colNames:\n", "        if (df[colName].dtypes == 'object'):\n", "            continue\n", "        #print(colName)\n", "        colValues = df[colName].values\n", "        #print(colValues)\n", "        #outCount = colOutCount(colValues)\n", "        #print(outCount)\n", "        dsRetValue[colName] = colOutCount(colValues)\n", "    return(dsRetValue)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["oulier index for column"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def colOutIndex(colValues):\n", "    \"\"\"\n", "    returns: \n", "        row index in the colName\n", "    usage: \n", "        colOutIndex(colValues)\n", "    \"\"\"\n", "    quartile_1, quartile_3 = np.percentile(colValues, [25, 75])\n", "    iqr = quartile_3 - quartile_1\n", "    lower_bound = quartile_1 - (iqr * 3.0)\n", "    upper_bound = quartile_3 + (iqr * 3.0)\n", "    ndOutData = np.where((colValues > upper_bound) | (colValues < lower_bound))\n", "    ndOutData = np.array(ndOutData)\n", "    return ndOutData"]}, {"cell_type": "markdown", "metadata": {}, "source": ["oulier index for data frame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def OutlierIndex(df): \n", "    \"\"\"\n", "    returns: \n", "        row index of outliers in each column of dataframe\n", "    usage: \n", "        OutlierIndex(df): \n", "    \"\"\"\n", "    colNames = df.columns\n", "    dsRetValue = pd.Series() \n", "    for colName in colNames:\n", "        if (df[colName].dtypes == 'object'):\n", "            continue\n", "        colValues = df[colName].values\n", "        dsRetValue[colName] = str(colOutIndex(colValues))\n", "    return(dsRetValue)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["outlier values for column "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def colOutValues(colValues):\n", "    \"\"\"\n", "    returns: \n", "        actual outliers values in the colName\n", "    usage: \n", "        colOutValues(colValues)\n", "    \"\"\"\n", "    quartile_1, quartile_3 = np.percentile(colValues, [25, 75])\n", "    iqr = quartile_3 - quartile_1\n", "    lower_bound = quartile_1 - (iqr * 3.0)\n", "    upper_bound = quartile_3 + (iqr * 3.0)\n", "    ndOutData = np.where((colValues > upper_bound) | (colValues < lower_bound))\n", "    ndOutData = np.array(colValues[ndOutData])\n", "    return ndOutData"]}, {"cell_type": "markdown", "metadata": {}, "source": ["outlier values for dataframe "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def OutlierValues(df): \n", "    \"\"\"\n", "    returns: \n", "        actual of outliers in each column of dataframe\n", "    usage: \n", "        OutlierValues(df): \n", "    \"\"\"\n", "    colNames = df.columns\n", "    dsRetValue = pd.Series() \n", "    for colName in colNames:\n", "        if (df[colName].dtypes == 'object'):\n", "            continue\n", "        colValues = df[colName].values\n", "        #print('Column: ', colName)\n", "        #strRetValue = strRetValue + colName + \" \" + \"\\n\"\n", "        #strRetValue = strRetValue + str(colOutValues(colValues)) + \" \\n\"\n", "        #print(colOutValues(colValues))\n", "        #print(\" \")\n", "        dsRetValue[colName] = str(colOutValues(colValues))\n", "    return(dsRetValue)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["outlier limits"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def OutlierLimits(colValues): \n", "    \"\"\"\n", "    returns: \n", "        upper boud & lower bound for array values or df[col].values \n", "    usage: \n", "        OutlierLimits(df[col].values): \n", "    \"\"\"\n", "    quartile_1, quartile_3 = np.percentile(colValues, [25, 75])\n", "    iqr = quartile_3 - quartile_1\n", "    lower_bound = quartile_1 - (iqr * 3.0)\n", "    upper_bound = quartile_3 + (iqr * 3.0)\n", "    return lower_bound, upper_bound"]}, {"cell_type": "markdown", "metadata": {}, "source": ["standardize data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def StandardizeData(df, colClass):\n", "    \"\"\"\n", "    desc:\n", "        standardize data - all cols of df will be Standardized except colClass \n", "        x_scaled = (x \u00e2\u20ac\u201d mean(x)) / stddev(x)\n", "        all values will be between 1 & -1\n", "    usage: \n", "        StandardizeData(df, colClass) \n", "    params:\n", "        df datarame, colClass - col to ignore while transformation  \n", "    \"\"\"\n", "    # preparing for standadrising\n", "    colNames = df.columns.tolist()\n", "    lstClass = df[colClass]\n", "    # standardizaion : \n", "    from sklearn.preprocessing import StandardScaler\n", "    scaler = StandardScaler()\n", "    # fit\n", "    ar = scaler.fit_transform(df)\n", "    # transform\n", "    df = pd.DataFrame(data=ar)\n", "    # # change as required\n", "    df.columns = colNames\n", "    df[colClass] = lstClass\n", "    return(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["normalize data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def NormalizeData(df, colClass):\n", "    \"\"\"\n", "    desc:\n", "        normalize data - all cols of df will be Normalized except colClass \n", "        x_scaled = (x-min(x)) / (max(x)\u00e2\u20ac\u201cmin(x))\n", "        all values will be between 0 & 1\n", "    usage: \n", "        NormalizeeData(df, colClass) \n", "    params:\n", "        df datarame, colClass - col to ignore while transformation  \n", "    \"\"\"\n", "    # preparing for normalization\n", "    colNames = df.columns.tolist()\n", "    lstClass = df[colClass]\n", "    from sklearn.preprocessing import MinMaxScaler\n", "    # normalizing the data\n", "    scaler = MinMaxScaler()\n", "    # fit\n", "    ar = scaler.fit_transform(df)\n", "    # transform\n", "    df = pd.DataFrame(data=ar)\n", "    # # change as required\n", "    df.columns = colNames\n", "    df[colClass] = lstClass\n", "    return(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Max Abs Scalaed Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def MaxAbsScaledData(df, colClass):\n", "    \"\"\"\n", "    desc:\n", "        MaxAbsScaled data - all cols of df will be MaxAbsScaled except colClass \n", "        x_scaled = x / max(abs(x))\n", "    Usage: \n", "        MaxAbsScaledData(df, colClass) \n", "    Params:\n", "        df datarame, colClass - col to ignore while transformation  \n", "    \"\"\"\n", "    # preparing for standadrising\n", "    colNames = df.columns.tolist()\n", "    lstClass = df[colClass]\n", "    # normalizing the data \n", "    from sklearn.preprocessing import MaxAbsScaler\n", "    scaler = MaxAbsScaler()\n", "    # fit\n", "    ar = scaler.fit_transform(df)\n", "    # transform\n", "    df = pd.DataFrame(data=ar)\n", "    # # change as required\n", "    df.columns = colNames\n", "    df[colClass] = lstClass\n", "    return(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["getFeatureScoresXTC - Extra Tree Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getFeatureScoresXTC(df, colClass):\n", "    \"\"\"\n", "    desc:\n", "        prints feature scores of all cols except colClass \n", "    usage: \n", "        getFeatureScoresXTC(df, colClass) \n", "    params:\n", "        df datarame, colClass - col to ignore while transformation  \n", "   \"\"\"\n", "    # make into array\n", "    #print(\"\\n*** Prepare Data ***\")\n", "    # store class variable  ... change as required\n", "    clsVars = colClass\n", "    allCols = df.columns.tolist()\n", "    #print(allCols)\n", "    allCols.remove(clsVars)\n", "    #print(allCols)\n", "    # split into X & y        \n", "    X = df[allCols].values\n", "    y = df[clsVars].values\n\n", "    # feature extraction with ExtraTreesClassifier\n", "    from sklearn.ensemble import ExtraTreesClassifier\n", "    # extraction\n", "    model = ExtraTreesClassifier(n_estimators=10, random_state=707)\n", "    model.fit(X, y)\n", "    #print(\"\\n*** Column Scores ***\")\n", "    # summarize scores\n", "    np.set_printoptions(precision=3)\n", "    #print(model.feature_importances_)\n", "    # data frame\n", "    dfm =  pd.DataFrame({'Cols':allCols, 'Imp':model.feature_importances_})  \n", "    dfm.sort_values(by='Imp', axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last') \n", "    return (dfm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["getFeatureScoresSKB - Select K Best"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getFeatureScoresSKB(df, colClass):\n", "    \"\"\"\n", "    desc:\n", "        prints feature scores of all cols except colClass \n", "    usage: \n", "        getFeatureScoresXTC(df, colClass) \n", "    params:\n", "        df datarame, colClass - col to ignore while transformation  \n", "    \"\"\"\n", "    # make into array\n", "    #print(\"\\n*** Prepare Data ***\")\n", "    # store class variable  ... change as required\n", "    clsVars = colClass\n", "    allCols = df.columns.tolist()\n", "    #print(allCols)\n", "    allCols.remove(clsVars)\n", "    #print(allCols)\n", "    # split into X & y        \n", "    X = df[allCols].values\n", "    y = df[clsVars].values\n", "    \n", "    # Feature extraction with selectBest\n", "    from sklearn.feature_selection import SelectKBest\n", "    from sklearn.feature_selection import f_classif\n", "    # feature extraction\n", "    model = SelectKBest(score_func=f_classif, k=4)\n", "    fit = model.fit(X, y)\n", "    # summarize scores\n", "    np.set_printoptions(precision=3)\n", "    #print(fit.scores_)\n", "    # data frame\n", "    dfm =  pd.DataFrame({'Cols':allCols, 'Imp':fit.scores_})  \n", "    dfm.sort_values(by='Imp', axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last') \n", "    return (dfm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["get OverSampleData"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getOverSamplerData(X,y): \n", "    \"\"\"\n", "    install:\n", "        !pip install -U imbalanced-learn\n", "    url:\n", "        https://pypi.org/project/imbalanced-learn/\n", "    desc:\n", "        Random Over Sampler ... \n", "        creates duplicate records of the lower sample\n", "        to match the sample size of highest size class\n", "    usage: \n", "        getOverSamplerData(X, y) ... requires standard X, y \n", "    \"\"\"\n", "    # import\n", "    from imblearn.over_sampling import RandomOverSampler\n", "    # create os object\n", "    os =  RandomOverSampler(random_state = 707)\n", "    # generate over sampled X, y\n", "    return (os.fit_resample(X, y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["get SMOTE Sampler Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getSmoteSamplerData(X,y): \n", "    \"\"\"\n", "    install:\n", "        !pip install -U imbalanced-learn\n", "    url:\n", "        https://pypi.org/project/imbalanced-learn/\n", "    desc:\n", "        SMOTE - Synthetic Minority Oversampling Technique \n", "        creates random new synthetic records\n", "        to match the sample size of highest size class\n", "    usage: \n", "        getSmoteSamplerData(X, y) ... requires standard X, y \n", "    \"\"\"\n", "    # import\n", "    from imblearn.over_sampling import SMOTE\n", "    # create smote object\n", "    sm = SMOTE(random_state = 707)\n", "    # generate over sampled X, y\n", "    return (sm.fit_resample(X, y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["get UnderSamplerData"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getUnderSamplerData(X,y): \n", "    \"\"\"\n", "    install:\n", "        !pip install -U imbalanced-learn\n", "    url:\n", "        https://pypi.org/project/imbalanced-learn/\n", "    desc:\n", "        Random Under Sampler ... \n", "        deletes records of the higher sample\n", "        to match the sample size of lowest size class\n", "    usage:  \n", "        getUnderSamplerData(X, y)\n", "    params:\n", "        requires standard X, y \n", "    \"\"\"\n", "    # import\n", "    from imblearn.under_sampling import RandomUnderSampler\n", "    # create os object\n", "    us =  RandomUnderSampler(random_state = 707, replacement=True)\n", "    # generate over sampled X, y\n", "    return (us.fit_resample(X, y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["one hot encoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def oheBind(pdf, encCol):\n", "    \"\"\"\n", "    desc:\n", "        One Hot Encoding \n", "        Col With Categoric Values A & B is converted to ColA & ColB with 0s & 1s\n", "    usage: \n", "        oheBind(pdf, encCol)\n", "    params:\n", "        pdf - data frame, encCol - column to be encoded\n", "    returns:\n", "        df with oheCols & encCol deleted\n", "    \"\"\"\n", "    ohe = pd.get_dummies(pdf[[encCol]])\n", "    #ohe.columns = pdf[encCol].unique()\n", "    rdf = pd.concat([pdf, ohe], axis=1)\n", "    rdf = rdf.drop(encCol, axis=1)\n", "    return(rdf)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}